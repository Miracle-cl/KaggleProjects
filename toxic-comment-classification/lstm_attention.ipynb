{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], 1),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1], 1),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        logits = K.dot(x, self.W) + self.b\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            a *= mask\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        att_weights = ai / K.sum(ai, axis=1, keepdims=True)\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        #if self.return_attention:\n",
    "            #return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "        #return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove Vectors\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.25\n",
    "rate_drop_dense = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## process texts in datasets\n",
    "import re\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal = re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "#regex to replace all numerics\n",
    "replace_numbers = re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    text = special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    text = replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('datasets/test.csv')\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation why the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired nown',\n",
       " 'daww he matches this background colour im seemingly stuck with thanks talk n january n n utc',\n",
       " 'hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info',\n",
       " ' more i cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport ',\n",
       " 'you sir are my hero any chance you remember what page thats on']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(comments + test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[675, 79, 1, 137, 129, 178, 29, 666, 4398, 9812, 1291, 84, 348, 52, 1951, 13200, 49, 6346, 16, 62, 2501, 145, 7, 2654, 33, 115, 1155, 15630, 2534, 4, 51, 53, 242, 1, 424, 31, 1, 60, 30, 139, 68, 3863, 12750], [54, 2736, 14, 1402, 3672, 68, 4561, 2508, 22, 96, 60, 12, 947, 12, 12, 211], [446, 389, 68, 122, 15, 253, 2, 82, 324, 43, 49, 9, 14, 568, 8, 2280, 492, 472, 105, 4, 561, 2, 37, 310, 137, 357, 3, 29, 60, 30, 54, 184, 2, 436, 61, 35, 1, 2273, 94, 1, 677, 475], [61, 7, 191, 98, 57, 317, 1331, 16, 1981, 7, 5334, 23, 1, 114, 2258, 59, 17, 483, 16, 27, 5, 3161, 3, 1256, 3, 9891, 7, 66, 1, 281, 87, 118, 12628, 36, 9, 52, 19, 42, 10, 1, 1410, 136, 1210, 698, 431, 1210, 313, 7, 39, 34, 9, 483, 16, 23, 3330, 308, 101, 112, 23, 6, 20, 57, 4302, 13, 2273, 478, 16, 281, 27, 107, 2, 34, 11, 220, 51, 263, 37, 72, 41, 515, 2, 17, 5, 5607, 16, 80, 13, 373, 36, 7, 598, 41, 87, 17, 5, 2045, 363, 5, 2844, 2833, 76, 43, 412, 10, 1, 472, 596, 887], [6, 1666, 19, 29, 3509, 57, 1011, 6, 545, 38, 30, 162, 16]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# {'the': 1,\n",
    "#  'to': 2,\n",
    "#  'of': 3,\n",
    "#  'and': 4,\n",
    "#  'a': 5,\n",
    "#  'you': 6,\n",
    "#  'i': 7,\n",
    "#  'is': 8, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392183"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 150)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 30606\n"
     ]
    }
   ],
   "source": [
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample train/validation data\n",
    "\n",
    "# np.random.permutation(10) -- array([4, 2, 3, 8, 7, 5, 9, 6, 1, 0])\n",
    "perm = np.random.permutation(len(data)) # 随机打乱顺序\n",
    "idx_train = perm[:int(len(data)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data)*(1-VALIDATION_SPLIT)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143613, 150) (143613, 6)\n"
     ]
    }
   ],
   "source": [
    "data_train = data[idx_train]\n",
    "labels_train = y[idx_train]\n",
    "print(data_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data[idx_val]\n",
    "labels_val = y[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 300)          30000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150, 300)          721200    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 150, 300)          0         \n",
      "_________________________________________________________________\n",
      "attention_6 (Attention)      (None, 300)               450       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 30,801,272\n",
      "Trainable params: 800,760\n",
      "Non-trainable params: 30,000,512\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences= embedding_layer(comment_input)\n",
    "x = lstm_layer(embedded_sequences)\n",
    "x = Dropout(rate_drop_dense)(x)\n",
    "merged = Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "merged = Dense(num_dense, activation='relu')(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "preds = Dense(6, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[comment_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 300)          30000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150, 300)          721200    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 150, 300)          0         \n",
      "_________________________________________________________________\n",
      "attention_4 (Attention)      (None, 300)               450       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 30,801,272\n",
      "Trainable params: 800,760\n",
      "Non-trainable params: 30,000,512\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import Sequential\n",
    "# lstm_att = Sequential()\n",
    "# lstm_att.add(embedding_layer)\n",
    "# lstm_att.add(lstm_layer)\n",
    "# lstm_att.add(Dropout(rate_drop_dense))\n",
    "# lstm_att.add(Attention(MAX_SEQUENCE_LENGTH))\n",
    "# lstm_att.add(Dense(num_dense, activation='relu'))\n",
    "# lstm_att.add(Dropout(rate_drop_dense))\n",
    "# lstm_att.add(BatchNormalization())\n",
    "# lstm_att.add(Dense(6, activation='sigmoid'))\n",
    "# lstm_att.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# print(lstm_att.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "bst_model_path = 'lstm_att.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0593 - acc: 0.9794 - val_loss: 0.0539 - val_acc: 0.9816\n",
      "Epoch 2/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0530 - acc: 0.9809 - val_loss: 0.0488 - val_acc: 0.9825\n",
      "Epoch 3/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0498 - acc: 0.9818 - val_loss: 0.0475 - val_acc: 0.9826\n",
      "Epoch 4/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0473 - acc: 0.9825 - val_loss: 0.0473 - val_acc: 0.9830\n",
      "Epoch 5/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0454 - acc: 0.9832 - val_loss: 0.0472 - val_acc: 0.9826\n",
      "Epoch 6/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0436 - acc: 0.9837 - val_loss: 0.0467 - val_acc: 0.9832\n",
      "Epoch 7/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0422 - acc: 0.9841 - val_loss: 0.0453 - val_acc: 0.9836\n",
      "Epoch 8/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0408 - acc: 0.9845 - val_loss: 0.0463 - val_acc: 0.9835\n",
      "Epoch 9/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0395 - acc: 0.9849 - val_loss: 0.0456 - val_acc: 0.9832\n",
      "Epoch 10/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0380 - acc: 0.9853 - val_loss: 0.0473 - val_acc: 0.9828\n",
      "Epoch 11/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0370 - acc: 0.9857 - val_loss: 0.0458 - val_acc: 0.9837\n",
      "Epoch 12/50\n",
      "143613/143613 [==============================] - 211s 1ms/step - loss: 0.0356 - acc: 0.9863 - val_loss: 0.0500 - val_acc: 0.9823\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(data_train, labels_train, \n",
    "                 epochs=50, \n",
    "                 batch_size=256,  \n",
    "                 shuffle=True,\n",
    "                 callbacks=[early_stopping, model_checkpoint], \n",
    "                 validation_data=(data_val, labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making the submission before fine-tuning\n",
      "153164/153164 [==============================] - 49s 322us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.999313</td>\n",
       "      <td>4.550171e-01</td>\n",
       "      <td>0.987431</td>\n",
       "      <td>4.261750e-02</td>\n",
       "      <td>0.959817</td>\n",
       "      <td>2.599539e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>4.555678e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.537656e-08</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.371404e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>3.824639e-05</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>1.019876e-06</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>4.238672e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>3.751143e-06</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>4.417281e-06</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>4.238053e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>4.177630e-06</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>4.474023e-07</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>7.391547e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene        threat    insult  \\\n",
       "0  00001cee341fdb12  0.999313  4.550171e-01  0.987431  4.261750e-02  0.959817   \n",
       "1  0000247867823ef7  0.000039  4.555678e-07  0.000001  1.537656e-08  0.000008   \n",
       "2  00013b17ad220c46  0.001096  3.824639e-05  0.000359  1.019876e-06  0.000141   \n",
       "3  00017563c3f7919a  0.000253  3.751143e-06  0.000054  4.417281e-06  0.000040   \n",
       "4  00017695ad8997eb  0.000455  4.177630e-06  0.000108  4.474023e-07  0.000015   \n",
       "\n",
       "   identity_hate  \n",
       "0   2.599539e-01  \n",
       "1   1.371404e-06  \n",
       "2   4.238672e-05  \n",
       "3   4.238053e-07  \n",
       "4   7.391547e-07  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make the submission\n",
    "\n",
    "print('Start making the submission before fine-tuning')\n",
    "\n",
    "y_test = model.predict([test_data], batch_size=1024, verbose=1)\n",
    "\n",
    "sample_submission = pd.read_csv(\"datasets/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "\n",
    "sample_submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('sample_submission_lstm_att.csv', index=False) #0.9785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
